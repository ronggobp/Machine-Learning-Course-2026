===============================================================================================================
Install & Login W&B (Tracking)
===============================================================================================================
!pip install wandb -qU
import wandb
# Link ini akan meminta login, kalian tinggal copy-paste API key dari web W&B
wandb.login()
----------------------------------------------------------------------------------------------------------------
/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\/'
  | |_| | '_ \/ _` / _` |  _/ -_)
wandb: (1) Create a W&B account
wandb: (2) Use an existing W&B account
wandb: (3) Don't visualize my results
wandb: Enter your choice: 2
wandb: You chose 'Use an existing W&B account'
wandb: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)
wandb: Create a new API key at: https://wandb.ai/authorize?ref=models
wandb: Store your API key securely and do not share it.
wandb: Paste your API key and hit enter: ··········
wandb: No netrc file found, creating one.
wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc
wandb: Currently logged in as: han-dev321 (han-dev321-stikomelrahma) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
True
===============================================================================================================


===============================================================================================================
Magic Demo 1 (Traditional ML)
===============================================================================================================
import time
import random
import wandb
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# 1. Inisialisasi
run = wandb.init(project="hello-ml-2026", name="iris-loop-demo")

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# 2. Simulasi Proses Training yang "Belajar"
# Kita berpura-pura menambah jumlah pohon (n_estimators) secara bertahap
for i in range(1, 11):
    n_trees = i * 10
    model = RandomForestClassifier(n_estimators=n_trees)
    model.fit(X, y)
    acc = model.score(X, y)

    # Log data setiap putaran
    wandb.log({"accuracy": acc, "num_trees": n_trees})
    print(f"Step {i}: Training dengan {n_trees} pohon, Akurasi: {acc}")
    time.sleep(1) # Memberi jeda agar bisa dilihat live di dashboard

wandb.finish()
----------------------------------------------------------------------------------
Finishing previous runs because reinit is set to 'default'.
View run iris-loop-demo at: https://wandb.ai/han-dev321-stikomelrahma/hello-ml-2026/runs/4ymff7o8
View project at: https://wandb.ai/han-dev321-stikomelrahma/hello-ml-2026
Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
Find logs at: ./wandb/run-20260212_053007-4ymff7o8/logs
Tracking run with wandb version 0.24.2
Run data is saved locally in /content/wandb/run-20260212_053130-ojagcvus
Syncing run iris-loop-demo to Weights & Biases (docs)
View project at https://wandb.ai/han-dev321-stikomelrahma/hello-ml-2026
View run at https://wandb.ai/han-dev321-stikomelrahma/hello-ml-2026/runs/ojagcvus
Step 1: Training dengan 10 pohon, Akurasi: 1.0
Step 2: Training dengan 20 pohon, Akurasi: 1.0
Step 3: Training dengan 30 pohon, Akurasi: 1.0
Step 4: Training dengan 40 pohon, Akurasi: 1.0
Step 5: Training dengan 50 pohon, Akurasi: 1.0
Step 6: Training dengan 60 pohon, Akurasi: 1.0
Step 7: Training dengan 70 pohon, Akurasi: 1.0
Step 8: Training dengan 80 pohon, Akurasi: 1.0
Step 9: Training dengan 90 pohon, Akurasi: 1.0
Step 10: Training dengan 100 pohon, Akurasi: 1.0
==============================================================================================================


===============================================================================================================
Magic Demo 2 (Modern AI/GenAI)
===============================================================================================================
from transformers import pipeline

# Kita spesifikasikan model yang paham banyak bahasa (multilingual)
# model ini jauh lebih akurat untuk teks Indonesia
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"

classifier = pipeline("sentiment-analysis", model=model_name)
res = classifier("indonesia akan membaik di waktu yang akan mendatang!")

print(f"Hasil Analisis Sentimen: {res}").
--------------------------------------------------------------------------------------
/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: 
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
config.json: 100%
 953/953 [00:00<00:00, 67.0kB/s]
model.safetensors: 100%
 669M/669M [00:06<00:00, 211MB/s]
Loading weights: 100%
 201/201 [00:00<00:00, 499.12it/s, Materializing param=classifier.weight]
tokenizer_config.json: 100%
 39.0/39.0 [00:00<00:00, 1.37kB/s]
vocab.txt: 
 872k/? [00:00<00:00, 11.3MB/s]
special_tokens_map.json: 100%
 112/112 [00:00<00:00, 9.99kB/s]
Hasil Analisis Sentimen: [{'label': '2 stars', 'score': 0.2595956325531006}].
